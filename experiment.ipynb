{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "temb_dim = 4\n",
    "# time_steps has to have dimension of (batch_size, )\n",
    "time_steps = torch.tensor([1, 2, 3, 4, 5], dtype = torch.float32)\n",
    "\n",
    "factor = 10000 ** ((torch.arange(\n",
    "        start = 0 , end = temb_dim // 2, dtype = torch.float32) / (temb_dim // 2) )\n",
    "        )\n",
    "\n",
    "t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
    "t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8415,  0.0100,  0.5403,  0.9999],\n",
       "        [ 0.9093,  0.0200, -0.4161,  0.9998],\n",
       "        [ 0.1411,  0.0300, -0.9900,  0.9996],\n",
       "        [-0.7568,  0.0400, -0.6536,  0.9992],\n",
       "        [-0.9589,  0.0500,  0.2837,  0.9988]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0100],\n",
       "        [2.0000, 0.0200],\n",
       "        [3.0000, 0.0300],\n",
       "        [4.0000, 0.0400],\n",
       "        [5.0000, 0.0500]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(start = 0 , end = temb_dim // 2, dtype = torch.float32) / (temb_dim // 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# Create a random input tensor with the shape [batch_size, in_channels, height, width]\n",
    "input_image = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# Apply the convolutional layer\n",
    "output = conv_layer(input_image)\n",
    "\n",
    "## the formulae for the output layer is given by \n",
    "\n",
    "# output_size = ((input_size - kernel_size + 2*padding) / stride) + 1\n",
    "\n",
    "# Print the shape of the output\n",
    "print(output.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "from turtle import pos\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "\n",
    "# Define the feature extractor (Fenc) using CLIP\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model_name=\"ViT-B/32\"):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.model, _ = clip.load(model_name, device='cpu')  # Load the CLIP model\n",
    "        self.model = self.model.visual  # Use the visual part of the model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define the position embedding (Emb)\n",
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding = nn.Parameter(torch.randn(1, embed_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _ = x.size()\n",
    "        embeddings = self.embedding.repeat(batch_size, 1)\n",
    "        return embeddings\n",
    "\n",
    "# Define the image input encoder\n",
    "class ImageInputEncoder(nn.Module):\n",
    "    def __init__(self, embed_size, model_name=\"ViT-B/32\"):\n",
    "        super(ImageInputEncoder, self).__init__()\n",
    "        self.feature_extractor = FeatureExtractor(model_name)\n",
    "        self.position_embedding = PositionEmbedding(embed_size)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)  # Extract features\n",
    "        B, C = features.size()\n",
    "        \n",
    "        pos_embeddings = self.position_embedding(features)  # Get position embeddings\n",
    "        print(pos_embeddings.shape)\n",
    "        features += pos_embeddings\n",
    "        \n",
    "        print(features.shape)\n",
    "        # Aggregate to get the image-level representation\n",
    "        image_condition = features.mean(dim=1)\n",
    "        \n",
    "        return image_condition\n",
    "\n",
    "# Example usage:\n",
    "# Create a random image tensor with shape [batch_size, channels, height, width]\n",
    "input_image = torch.randn(8, 3, 224, 224)  # Example input\n",
    "\n",
    "# Define the model and forward pass\n",
    "embed_size = 512  # Example embedding size for CLIP ViT-B/32\n",
    "model = ImageInputEncoder(embed_size, model_name=\"ViT-B/32\")\n",
    "output = model(input_image)\n",
    "\n",
    "print(output.shape)  # Should print: torch.Size([8, 512])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "\n",
    "# Define the feature extractor (Fenc) using CLIP\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model_name=\"ViT-B/32\"):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.model, _ = clip.load(model_name, device='cpu')  # Load the CLIP model\n",
    "        self.model = self.model.visual  # Use the visual part of the model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define the position embedding (Emb)\n",
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding = nn.Parameter(torch.randn(1, 1, embed_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_patches, _ = x.size()\n",
    "        embeddings = self.embedding.expand(batch_size, num_patches, -1)\n",
    "        return embeddings\n",
    "\n",
    "# Define the image input encoder\n",
    "class ImageInputEncoder(nn.Module):\n",
    "    def __init__(self, embed_size, model_name=\"ViT-B/32\"):\n",
    "        super(ImageInputEncoder, self).__init__()\n",
    "        self.feature_extractor = FeatureExtractor(model_name)\n",
    "        self.position_embedding = PositionEmbedding(embed_size)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)  # Extract features\n",
    "        print(features.shape)\n",
    "        B, C = features.size()\n",
    "        \n",
    "        # Reshape features to match positional embeddings dimensions\n",
    "        # features = features.view(B, 1, C)\n",
    "        # # print(features.shape)\n",
    "        \n",
    "        # pos_embeddings = self.position_embedding(features)  # Get position embeddings\n",
    "        # features += pos_embeddings\n",
    "        \n",
    "        # print(pos_embeddings.shape)\n",
    "        # # Aggregate to get the image-level representation\n",
    "        # image_condition = features.mean(dim=1)\n",
    "        \n",
    "        return features\n",
    "\n",
    "# Example usage:\n",
    "# Create a random image tensor with shape [batch_size, channels, height, width]\n",
    "input_image = torch.randn(8, 3, 224, 224)  # Example input\n",
    "\n",
    "# Define the model and forward pass\n",
    "embed_size = 512  # Example embedding size for CLIP ViT-B/32\n",
    "model = ImageInputEncoder(embed_size, model_name=\"ViT-B/32\")\n",
    "output = model(input_image)\n",
    "\n",
    "print(output.shape)  # Should print: torch.Size([8, 512])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 77, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "# Define the function to get the tokenizer and model based on the model type\n",
    "def get_tokenizer_and_model(model_type, device, eval_mode=True):\n",
    "    if model_type == 'bert':\n",
    "        text_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        text_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "    elif model_type == 'clip':\n",
    "        text_tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch16')\n",
    "        text_model = CLIPTextModel.from_pretrained('openai/clip-vit-base-patch16').to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model_type: {model_type}\")\n",
    "    \n",
    "    if eval_mode:\n",
    "        text_model.eval()\n",
    "    \n",
    "    return text_tokenizer, text_model\n",
    "\n",
    "# Define the function to get text representation\n",
    "def get_text_representation(text, text_tokenizer, text_model, device,\n",
    "                            truncation=True,\n",
    "                            padding='max_length',\n",
    "                            max_length=77):\n",
    "    token_output = text_tokenizer(text,\n",
    "                                  truncation=truncation,\n",
    "                                  padding=padding,\n",
    "                                  return_attention_mask=True,\n",
    "                                  max_length=max_length,\n",
    "                                  return_tensors='pt')  # Return PyTorch tensors\n",
    "    tokens_tensor = token_output['input_ids'].to(device)\n",
    "    mask_tensor = token_output['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        text_embed = text_model(input_ids=tokens_tensor, attention_mask=mask_tensor).last_hidden_state\n",
    "    \n",
    "    return text_embed\n",
    "\n",
    "# Example usage\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "text = [\"A photo of a cat\", \"A photo of a dog\"]\n",
    "model_type = 'clip'\n",
    "text_tokenizer, text_model = get_tokenizer_and_model(model_type, device)\n",
    "text_embed = get_text_representation(text, text_tokenizer, text_model, device)\n",
    "print(text_embed.shape)  # Should print: torch.Size([1, 77, 512])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tejanagubandi/Desktop/projects/CTIDiffusion/CTIDiffusion/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "image_features = model.get_image_features(**inputs)\n",
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO we need to add the dataset class for IAM dataset \n",
    "#TODO we need to prepare the dataset get method and also load images\n",
    "\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from Utils.diffusion_utils import load_latents\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "# IAM Dataset Experimenting here Need to complete this\n",
    "\n",
    "class IAM(Dataset):\n",
    "    r\"\"\"\n",
    "    Celeb dataset will by default centre crop and resize the images.\n",
    "    This can be replaced by any other dataset. As long as all the images\n",
    "    are under one directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, split, im_path, im_size=256, im_channels=3, im_ext='jpg',\n",
    "                 use_latents=False, latent_path=None, condition_config=None):\n",
    "        self.split = split\n",
    "        self.im_size = im_size\n",
    "        self.im_channels = im_channels\n",
    "        self.im_ext = im_ext\n",
    "        self.im_path = im_path\n",
    "        self.latent_maps = None\n",
    "        self.use_latents = False\n",
    "        \n",
    "        self.condition_types = [] if condition_config is None else condition_config['condition_types']\n",
    "        \n",
    "        self.idx_to_cls_map = {}\n",
    "        self.cls_to_idx_map ={}\n",
    "        \n",
    "        if 'image' in self.condition_types:\n",
    "            self.mask_channels = condition_config['image_condition_config']['image_condition_input_channels']\n",
    "            self.mask_h = condition_config['image_condition_config']['image_condition_h']\n",
    "            self.mask_w = condition_config['image_condition_config']['image_condition_w']\n",
    "            \n",
    "        self.images, self.texts, self.masks = self.load_images(im_path)\n",
    "        \n",
    "        # Whether to load images or to load latents\n",
    "        if use_latents and latent_path is not None:\n",
    "            latent_maps = load_latents(latent_path)\n",
    "            if len(latent_maps) == len(self.images):\n",
    "                self.use_latents = True\n",
    "                self.latent_maps = latent_maps\n",
    "                print('Found {} latents'.format(len(self.latent_maps)))\n",
    "            else:\n",
    "                print('Latents not found')\n",
    "    \n",
    "    def load_images(self, im_path):\n",
    "        r\"\"\"\n",
    "        Gets all images from the path specified\n",
    "        and stacks them all up\n",
    "        \"\"\"\n",
    "        assert os.path.exists(im_path), \"images path {} does not exist\".format(im_path)\n",
    "        ims = []\n",
    "        fnames = glob.glob(os.path.join(im_path, 'CelebA-HQ-img/*.{}'.format('png')))\n",
    "        fnames += glob.glob(os.path.join(im_path, 'CelebA-HQ-img/*.{}'.format('jpg')))\n",
    "        fnames += glob.glob(os.path.join(im_path, 'CelebA-HQ-img/*.{}'.format('jpeg')))\n",
    "        texts = []\n",
    "        masks = []\n",
    "        \n",
    "        if 'image' in self.condition_types:\n",
    "            label_list = ['skin', 'nose', 'eye_g', 'l_eye', 'r_eye', 'l_brow', 'r_brow', 'l_ear', 'r_ear', 'mouth',\n",
    "                          'u_lip', 'l_lip', 'hair', 'hat', 'ear_r', 'neck_l', 'neck', 'cloth']\n",
    "            self.idx_to_cls_map = {idx: label_list[idx] for idx in range(len(label_list))}\n",
    "            self.cls_to_idx_map = {label_list[idx]: idx for idx in range(len(label_list))}\n",
    "        \n",
    "        for fname in tqdm(fnames):\n",
    "            ims.append(fname)\n",
    "            \n",
    "            if 'text' in self.condition_types:\n",
    "                im_name = os.path.split(fname)[1].split('.')[0]\n",
    "                captions_im = []\n",
    "                with open(os.path.join(im_path, 'celeba-caption/{}.txt'.format(im_name))) as f:\n",
    "                    for line in f.readlines():\n",
    "                        captions_im.append(line.strip())\n",
    "                texts.append(captions_im)\n",
    "                \n",
    "            if 'image' in self.condition_types:\n",
    "                im_name = int(os.path.split(fname)[1].split('.')[0])\n",
    "                masks.append(os.path.join(im_path, 'CelebAMask-HQ-mask', '{}.png'.format(im_name)))\n",
    "        if 'text' in self.condition_types:\n",
    "            assert len(texts) == len(ims), \"Condition Type Text but could not find captions for all images\"\n",
    "        if 'image' in self.condition_types:\n",
    "            assert len(masks) == len(ims), \"Condition Type Image but could not find masks for all images\"\n",
    "        print('Found {} images'.format(len(ims)))\n",
    "        print('Found {} masks'.format(len(masks)))\n",
    "        print('Found {} captions'.format(len(texts)))\n",
    "        return ims, texts, masks\n",
    "    \n",
    "    def get_mask(self, index):\n",
    "        r\"\"\"\n",
    "        Method to get the mask of WxH\n",
    "        for given index and convert it into\n",
    "        Classes x W x H mask image\n",
    "        :param index:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        mask_im = Image.open(self.masks[index])\n",
    "        mask_im = np.array(mask_im)\n",
    "        im_base = np.zeros((self.mask_h, self.mask_w, self.mask_channels))\n",
    "        for orig_idx in range(len(self.idx_to_cls_map)):\n",
    "            im_base[mask_im == (orig_idx+1), orig_idx] = 1\n",
    "        mask = torch.from_numpy(im_base).permute(2, 0, 1).float()\n",
    "        return mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ######## Set Conditioning Info ########\n",
    "        cond_inputs = {}\n",
    "        if 'text' in self.condition_types:\n",
    "            cond_inputs['text'] = random.sample(self.texts[index], k=1)[0]\n",
    "        if 'image' in self.condition_types:\n",
    "            mask = self.get_mask(index)\n",
    "            cond_inputs['image'] = mask\n",
    "        #######################################\n",
    "        \n",
    "        if self.use_latents:\n",
    "            latent = self.latent_maps[self.images[index]]\n",
    "            if len(self.condition_types) == 0:\n",
    "                return latent\n",
    "            else:\n",
    "                return latent, cond_inputs\n",
    "        else:\n",
    "            im = Image.open(self.images[index])\n",
    "            im_tensor = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.Resize(self.im_size),\n",
    "                torchvision.transforms.CenterCrop(self.im_size),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "            ])(im)\n",
    "            im.close()\n",
    "        \n",
    "            # Convert input to -1 to 1 range.\n",
    "            im_tensor = (2 * im_tensor) - 1\n",
    "            if len(self.condition_types) == 0:\n",
    "                return im_tensor\n",
    "            else:\n",
    "                return im_tensor, cond_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a01-000u-00-00', 'ok', '154', '408', '768', '27', '51', 'AT', 'A']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_path = 'iam-handwriting-words-da/iam_words/words/a01/a01-000u/a01-000u-00-00.png'\n",
    "text_string =  \"a01-000u-00-00 ok 154 408 768 27 51 AT A\"\n",
    "\n",
    "split_string = text_string.split(' ')\n",
    "split_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = split_string[-1]\n",
    "image_name = split_string[0]\n",
    "image_name_folder_1 = image_name.split('-')[0]\n",
    "image_name_folder_2 = image_name.split('-')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = split_string[0]\n",
    "image_name_folder_1 = image_name.split('-')[0]\n",
    "image_name_folder_2 = image_name_folder_1 + '-' + image_name.split('-')[1] \n",
    "image_final_folder = image_name_folder_1 + '/' + image_name_folder_2 + '/' + image_name\n",
    "\n",
    "im_path = '/iam-handwriting-words-da'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "image_path = os.path.join(im_path, 'iam_words/words/{}.{}'.format(image_final_folder, 'png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/iam-handwriting-words-da/iam_words/words/a01/a01-000u/a01-000u-00-00.png'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tejanagubandi/Desktop/projects/CTIDiffusion/CTIDiffusion/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "{'dataset_params': {'im_path': '/iam_words', 'im_channels': 3, 'im_size': 256, 'name': 'IAMHandwriting'}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.0015, 'beta_end': 0.0195}, 'ldm_params': {'down_channels': [128, 256, 256, 256], 'mid_channels': [256, 256], 'down_sample': [False, False, False], 'attn_down': [True, True, True], 'time_emb_dim': 256, 'norm_channels': 32, 'num_heads': 16, 'conv_out_channels': 128, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2, 'condition_config': {'condition_types': ['text', 'image', 'style'], 'text_condition_config': {'text_embed_model': 'clip', 'train_text_embed_model': False, 'text_embed_dim': 512, 'cond_drop_prob': 0.1}, 'image_condition_config': {'image_embed_model': 'clip', 'train_image_embed_model': False, 'image_embed_dim': 512, 'cond_drop_prob': 0.1}}}, 'autoencoder_params': {'z_channels': 3, 'codebook_size': 20, 'down_channels': [32, 64, 128], 'mid_channels': [128, 128], 'down_sample': [True, True], 'attn_down': [False, False], 'norm_channels': 32, 'num_heads': 16, 'num_down_layers': 1, 'num_mid_layers': 1, 'num_up_layers': 1}, 'train_params': {'seed': 1111, 'task_name': 'IAMHandwriting', 'ldm_batch_size': 64, 'autoencoder_batch_size': 64, 'disc_start': 1000, 'disc_weight': 0.5, 'codebook_weight': 1, 'commitment_beta': 0.2, 'perceptual_weight': 1, 'kl_weight': 5e-06, 'ldm_epochs': 100, 'autoencoder_epochs': 10, 'num_samples': 25, 'num_grid_rows': 5, 'ldm_lr': 1e-05, 'autoencoder_lr': 0.0001, 'autoencoder_acc_steps': 1, 'autoencoder_img_save_steps': 8, 'save_latents': False, 'ldm_ckpt_name': 'cond_text.pth'}}\n",
      "/Users/tejanagubandi/Desktop/projects/CTIDiffusion/CTIDiffusion/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tejanagubandi/Desktop/projects/CTIDiffusion/Training/train.py\", line 155, in <module>\n",
      "    train(args)\n",
      "  File \"/Users/tejanagubandi/Desktop/projects/CTIDiffusion/Training/train.py\", line 67, in train\n",
      "    image_model, image_processor = get_image_model_processor(condition_config['image_condition_config']['image_embed_model'], device = device)\n",
      "  File \"/Users/tejanagubandi/Desktop/projects/CTIDiffusion/utils/pre_trained_utils.py\", line 23, in get_image_model_processor\n",
      "    image_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
      "  File \"/Users/tejanagubandi/Desktop/projects/CTIDiffusion/CTIDiffusion/lib/python3.9/site-packages/transformers/models/auto/processing_auto.py\", line 321, in from_pretrained\n",
      "    return PROCESSOR_MAPPING[type(config)].from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/Users/tejanagubandi/Desktop/projects/CTIDiffusion/CTIDiffusion/lib/python3.9/site-packages/transformers/processing_utils.py\", line 468, in from_pretrained\n",
      "    return cls.from_args_and_dict(args, processor_dict, **kwargs)\n",
      "  File \"/Users/tejanagubandi/Desktop/projects/CTIDiffusion/CTIDiffusion/lib/python3.9/site-packages/transformers/processing_utils.py\", line 400, in from_args_and_dict\n",
      "    logger.info(f\"Processor {processor}\")\n",
      "  File \"/Users/tejanagubandi/Desktop/projects/CTIDiffusion/CTIDiffusion/lib/python3.9/site-packages/transformers/processing_utils.py\", line 165, in __repr__\n",
      "    return f\"{self.__class__.__name__}:\\n{attributes_repr}\\n\\n{self.to_json_string()}\"\n",
      "  File \"/Users/tejanagubandi/Desktop/projects/CTIDiffusion/CTIDiffusion/lib/python3.9/site-packages/transformers/processing_utils.py\", line 147, in to_json_string\n",
      "    dictionary = self.to_dict()\n",
      "  File \"/Users/tejanagubandi/Desktop/projects/CTIDiffusion/CTIDiffusion/lib/python3.9/site-packages/transformers/processing_utils.py\", line 109, in to_dict\n",
      "    output = copy.deepcopy(self.__dict__)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/copy.py\", line 146, in deepcopy\n",
      "    y = copier(x, memo)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/copy.py\", line 230, in _deepcopy_dict\n",
      "    y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/copy.py\", line 172, in deepcopy\n",
      "    y = _reconstruct(x, memo, *rv)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/copy.py\", line 270, in _reconstruct\n",
      "    state = deepcopy(state, memo)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/copy.py\", line 146, in deepcopy\n",
      "    y = copier(x, memo)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/copy.py\", line 230, in _deepcopy_dict\n",
      "    y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/copy.py\", line 172, in deepcopy\n",
      "    y = _reconstruct(x, memo, *rv)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/copy.py\", line 264, in _reconstruct\n",
      "    y = func(*args)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/copy.py\", line 263, in <genexpr>\n",
      "    args = (deepcopy(arg, memo) for arg in args)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/copy.py\", line 161, in deepcopy\n",
      "    rv = reductor(4)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python3 /Users/tejanagubandi/Desktop/projects/CTIDiffusion/Training/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CTIDiffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
